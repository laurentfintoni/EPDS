{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Scraping**\n",
    "\n",
    "This notebook includes the code used to scrape the description pages for both Rhizome and MOMA websites. The resulting data is used for some visualisations as well as storytelling. For Rhizome extracting data was also deemed important to help create medium information for the artworks (based on certain keywords such as html, java, flash etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and path \n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "path = '/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 2/EPUB/PROJECT/EPDS/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rhizome Scraping**\n",
    "- After some trials with the html construction of the Rhizome artwork pages we decided to grab the div that includes all three possible descriptions (summary, artist statement, description) and clean the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape summaries, descriptions and artists statements from Rhizome website \n",
    "def url_to_text_rhizome(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    accordion = [p.text.strip() for p in soup.find(id=\"AccordionDescriptionBody\").find_all('div')]\n",
    "    print(url)\n",
    "    return accordion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab URLs from artworks DF, scrape them and return them back to the DF\n",
    "rhz_artworks_extra = pd.read_pickle(path+'Rhizome_data/rhizome_artworks_extra.pkl')\n",
    "urls = rhz_artworks_extra['URL'].to_list()\n",
    "scrapes = [url_to_text_rhizome(u) for u in urls]\n",
    "rhz_artworks_extra_text = rhz_artworks_extra.copy()\n",
    "rhz_artworks_extra_text['Text'] = pd.Series(scrapes)\n",
    "#fix an erroneous ID in original first round of scraping \n",
    "rhz_artworks_extra_text.loc[777, 'ID'] = '926, 1268'\n",
    "rhz_artworks_extra_text = rhz_artworks_extra_text.astype(str)\n",
    "rhz_artworks_extra_text.to_pickle(path+'Rhizome_data/rhizome_artworks_extra_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOMA Scraping**\n",
    "- For MOMA URLs there was only one possible description on the page but the containing div is repeated elsewhereso we used its parent container to only extract what we needed \n",
    "- We show the process for one dept only, but this was repeated for all of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load department DFs\n",
    "moma_arch_cont = pd.read_pickle('MOMA_data/pickle/departments/architecture_design_cont.pkl')\n",
    "moma_arch_mod = pd.read_pickle('MOMA_data/pickle/departments/architecture_design_mod.pkl')\n",
    "moma_design_cont = pd.read_pickle('MOMA_data/pickle/departments/architecture_design_img_cont.pkl')\n",
    "moma_design_mod = pd.read_pickle('MOMA_data/pickle/departments/architecture_design_img_mod.pkl')\n",
    "moma_draw_cont = pd.read_pickle('MOMA_data/pickle/departments/draws_prints_cont.pkl')\n",
    "moma_draw_mod = pd.read_pickle('MOMA_data/pickle/departments/draws_prints_mod.pkl')\n",
    "moma_films_cont = pd.read_pickle('MOMA_data/pickle/departments/films_cont.pkl')\n",
    "moma_films_mod = pd.read_pickle('MOMA_data/pickle/departments/films_mod.pkl')\n",
    "moma_fluxus_cont = pd.read_pickle('MOMA_data/pickle/departments/fluxus_cont.pkl')\n",
    "moma_fluxus_mod = pd.read_pickle('MOMA_data/pickle/departments/fluxus_mod.pkl')\n",
    "moma_media_cont = pd.read_pickle('MOMA_data/pickle/departments/media_perf_cont.pkl')\n",
    "moma_media_mod = pd.read_pickle('MOMA_data/pickle/departments/media_perf_mod.pkl')\n",
    "moma_paint_cont = pd.read_pickle('MOMA_data/pickle/departments/paint_sculp_cont.pkl')\n",
    "moma_paint_mod = pd.read_pickle('MOMA_data/pickle/departments/paint_sculp_mod.pkl')\n",
    "moma_photo_cont = pd.read_pickle('MOMA_data/pickle/departments/photo_cont.pkl')\n",
    "moma_photo_mod = pd.read_pickle('MOMA_data/pickle/departments/photo_mod.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract text if available w/ exclusions for 404 status, missing URLs, and pages w/ no despcription\n",
    "def url_to_text_moma(url):\n",
    "    if url != 'missing':\n",
    "        page = requests.get(url)\n",
    "        status = page.status_code\n",
    "        if status != 404:\n",
    "            soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "            try:\n",
    "                if soup.find(class_=\"uneven-columns--work\").find(class_='main-content') is not None:\n",
    "                    text = soup.find(class_=\"uneven-columns--work\").find(class_='main-content').find_all('p')\n",
    "                else:\n",
    "                    text = ''\n",
    "            except AttributeError:\n",
    "                text = ''\n",
    "                pass\n",
    "        else:\n",
    "            text = '404'\n",
    "    else:\n",
    "        text = 'missing'     \n",
    "    print(url)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grap all links from a dept as a list \n",
    "links = moma_photo_mod['URL'].to_list()\n",
    "#process links w/ function \n",
    "moma_photo_to_add = [url_to_text_moma(u) for u in links]\n",
    "#add results back to a copy of the original DF\n",
    "moma_photo_mod_text = moma_photo_mod.copy()\n",
    "moma_photo_mod_text['Text'] = moma_photo_to_add\n",
    "moma_photo_mod_text['Text'] = moma_photo_mod_text['Text'].astype(str)\n",
    "moma_photo_mod_text.to_pickle('/Users/laurentfintoni/Desktop/University/COURSE DOCS/YEAR 2/EPUB/PROJECT/EPDS/MOMA_data/pickle/departments/photo_mod_text.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rhizome Cleaning**\n",
    "- Clean the text of html parsing errors \n",
    "- Analyse most common words for custom stopwords\n",
    "- Run text through nltk english stopwords and custom ones \n",
    "- Extract 20 keywords for each text and add them as a new column (to use for assessing mediums of artworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhz_artworks_extra_text = pd.read_pickle(path+'Rhizome_data/rhizome_artworks_extra_text.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of strings to remove from all scrapes\n",
    "remove = ['description edit\\\\\\\\n\\\\\\\\n\\\\\\\\t\\\\\\\\t\\\\\\\\t\\\\\\\\t', '[', ']', '\\\\\\\\n\\\\\\\\n\\\\\\\\n\\\\\\\\', '\\'description edit\\', ', 'edit\\\\\\\\n\\\\\\\\n', '\\'summary edit\\', ', 'tttt', 'nn']\n",
    "for char in remove:\n",
    "    rhz_artworks_extra_text['Text'] = rhz_artworks_extra_text['Text'].str.replace(char, '')\n",
    "\n",
    "#export cleaned version to pickle\n",
    "rhz_artworks_extra_text_clean = rhz_artworks_extra_text.copy()\n",
    "rhz_artworks_extra_text_clean.to_pickle(path+'Rhizome_data/rhizome_artworks_extra_text_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top 50 keywords \n",
    "common = pd.Series(' '.join(rhz_artworks_extra_text_clean['Text']).split()).value_counts()[:50]\n",
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for cleaning and keyword extraction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words and add custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words =  ['the', 'a ', 'rhizome', '\\'attributed', 'summary', 'to:', '\\'inception:', 'staff\\',', '\\'attribution:', '\\'summary', 'staffinception:', '-', '2021\\',', '\\'legacy', '2001\\',']\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords from cleanedtext \n",
    "rhz_artworks_extra_text_clean['Text'] = rhz_artworks_extra_text_clean['Text'].str.lower()\n",
    "rhz_artworks_extra_text_clean['Text'] = rhz_artworks_extra_text_clean['Text'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check top 50 keywords again\n",
    "common_clean = pd.Series(' '.join(rhz_artworks_extra_text_clean['Text']).split()).value_counts()[:50]\n",
    "common_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract keywords and pass them to a new column\n",
    "def get_keywords(row):\n",
    "    some_text = row['Text']\n",
    "    tokens = nltk.tokenize.word_tokenize(some_text)\n",
    "    keywords = [keyword for keyword in tokens if keyword.isalpha() and not keyword in stop_words]\n",
    "    keywords_string = ', '.join(keywords[0:20])\n",
    "    return keywords_string\n",
    "\n",
    "rhz_artworks_extra_text_clean['Keywords'] = rhz_artworks_extra_text_clean.apply(lambda row:get_keywords(row), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to new pickle for reuse\n",
    "rhz_artworks_extra_text_clean.to_pickle(path+'Rhizome_data/rhizome_artworks_extra_text_clean_stop_keywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhz_artworks_extra_text_clean.loc[rhz_artworks_extra_text_clean['Keywords'].str.contains('html', case=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MOMA Cleaning**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
